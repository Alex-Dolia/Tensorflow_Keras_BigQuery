{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd241de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tf.Tensor(\n",
      "[[b'( 1 OR 2 )  AND 3 OR NOT ( 4 OR 5 )']\n",
      " [b'( 10 OR 20 ) AND 30 OR NOT ( 40 OR 50 )']\n",
      " [b'( 100 OR 200 ) AND 300 OR NOT ( 400 OR 500 )']], shape=(3, 1), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=string, numpy=\n",
       "array([[b' 1 2 OR 3 AND 4 5 OR NOT OR'],\n",
       "       [b' 10 20 OR 30 AND 40 50 OR NOT OR'],\n",
       "       [b' 100 200 OR 300 AND 400 500 OR NOT OR']], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next NN type prediction:\n",
      "call precision:  Tensor(\"Placeholder:0\", shape=(None, 1), dtype=string)\n",
      "!!!!!!!!!!!!!! prediction:\n",
      "call precision:  Tensor(\"IteratorGetNext:0\", shape=(None, 1), dtype=string)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002401B913940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[[b' 1 2 OR 3 AND 4 5 OR NOT OR']\n",
      " [b' 10 20 OR 30 AND 40 50 OR NOT OR']\n",
      " [b' 100 200 OR 300 AND 400 500 OR NOT OR']]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import  tensorflow.keras.backend as K\n",
    "import  tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input\n",
    "#\n",
    "from tensorflow.keras.models import Model\n",
    "#\n",
    "########################### fixed size stack\n",
    "#https://favtutor.com/blogs/infix-to-postfix-conversion\n",
    "#@tf.keras.utils.register_keras_serializable()\n",
    "class infixToPostfix_AD(keras.layers.Layer):\n",
    "      def __init__(self, **kwargs):\n",
    "          #\n",
    "          super(infixToPostfix_AD, self).__init__(**kwargs)\n",
    "      #  \n",
    "      # about get_config  see the following link:\n",
    "      # https://towardsdatascience.com/how-to-write-a-custom-keras-model-so-that-it-can-be-deployed-for-serving-7d81ace4a1f8\n",
    "      def get_config(self):\n",
    "          config = super().get_config()\n",
    "          # save constructor args\n",
    "          #config['input_dim'] = self.input_dim\n",
    "          # \n",
    "          return config\n",
    "      #\n",
    "      #@tf.function\n",
    "      def call(self, inputs):\n",
    "          def infixToPostfix_FUN_AD(inputs): \n",
    "              inputs = tf.squeeze(inputs)\n",
    "              # input is tensor of strings, for example:\n",
    "              # inputs = tf.constant([[\"( 1 OR 2 )  AND 3 OR 4\"],[\"( 10 OR 20 ) AND 30 OR 40\"], [\"( 100 OR 200 ) AND 300 OR 400\"]])\n",
    "              #\n",
    "              def remove_extra_spaces(a):\n",
    "                  a = tf.strings.strip(a)\n",
    "                  a = tf.strings.regex_replace(a, ' +', \" \")\n",
    "                  return a\n",
    "              #\n",
    "              def stack_not_empty(stack_top_index):\n",
    "                  return tf.math.greater_equal(stack_top_index, 0)\n",
    "              #\n",
    "              def Original_push_stack(stack, update, number_of_rows, stack_top_index):\n",
    "                  #  \n",
    "                  print(\"push, number_of_rows: \", number_of_rows)\n",
    "                  update = tf.reshape(update, tf.shape(stack)[0])\n",
    "                  #\n",
    "                  stack_top_index = tf.add(stack_top_index, 1)\n",
    "                  #\n",
    "                  rr      = tf.reshape(tf.range(tf.shape(stack)[0]), (number_of_rows, 1))\n",
    "                  cc      = tf.repeat([[stack_top_index]], tf.shape(stack)[0], axis = 0)\n",
    "                  indeces = tf.concat([rr, cc], axis = 1 )\n",
    "                  #\n",
    "                  return stack_top_index, tf.tensor_scatter_nd_update(stack, indeces, update) \n",
    "              #  \n",
    "              def push_stack(stack, update, number_of_rows, stack_top_index):\n",
    "                  #  \n",
    "                  update = tf.squeeze(update)\n",
    "                  #\n",
    "                  #update = tf.reshape(update, (tf.size(update))  )\n",
    "                  #\n",
    "                  stack_top_index = tf.add(stack_top_index, 1)\n",
    "                  #\n",
    "                  rr      = tf.reshape(tf.range(tf.size(update)), (-1, 1))\n",
    "                  cc      = tf.repeat([[stack_top_index]], number_of_rows, axis = 0)\n",
    "                  indeces = tf.concat([rr, cc], axis = 1 )\n",
    "                  #\n",
    "                  return stack_top_index, tf.tensor_scatter_nd_update(stack, indeces, update) \n",
    "              #\n",
    "              #\n",
    "              def pop_stack(stack, number_of_rows, stack_top_index):\n",
    "                  #\n",
    "                  if tf.greater_equal(stack_top_index, 0):  \n",
    "                     stack_2D_top = tf.slice(stack, [0, stack_top_index], [number_of_rows, 1])\n",
    "                     stack_2D_top = tf.reshape(stack_2D_top, [number_of_rows, 1])\n",
    "                     #\n",
    "                     stack_top_index = tf.add(stack_top_index, -1)\n",
    "                     #\n",
    "                  else:\n",
    "                     stack_2D_top = \"\"\n",
    "                  return stack_top_index, stack_2D_top \n",
    "              #\n",
    "              def output_update(outputs, stack_2D_top, number_of_rows):\n",
    "                  #\n",
    "                  outputs = tf.concat([outputs, stack_2D_top], axis = 1) \n",
    "                  outputs = tf.map_fn(lambda x: tf.strings.join([tf.gather(x, 0), \" \", tf.gather(x, 1)]), outputs)  \n",
    "                  #\n",
    "                  outputs = tf.reshape(outputs, [number_of_rows, 1])\n",
    "                  #\n",
    "                  return outputs\n",
    "              #\n",
    "              def top_one_row_stack(stack, stack_top_index):\n",
    "                  #\n",
    "                  stack_row = tf.gather(stack, 0)\n",
    "                  #\n",
    "                  if stack_not_empty(stack_top_index):  \n",
    "                     stack_top = tf.gather(stack_row, stack_top_index)\n",
    "                  else:\n",
    "                     stack_top = \"ARBITRARY VALUE\"\n",
    "                  #\n",
    "                  return stack_top\n",
    "                \n",
    "              # table has priority levels: {\"OR\": 1, \"AND\": 2, \"NOT\": 3}\n",
    "              keys_tensor = tf.constant(['OR', 'AND', 'NOT'])\n",
    "              vals_tensor = tf.constant([1, 2, 3])\n",
    "              input_tensor = tf.constant(['NOT'])\n",
    "              table = tf.lookup.StaticHashTable(\n",
    "                      tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n",
    "                      default_value=-1)\n",
    "              #\n",
    "              # remove more than 1 spaces in a row and also beginning and ending spaces\n",
    "              inputs = tf.map_fn(lambda x: remove_extra_spaces(x), inputs)\n",
    "              #\n",
    "              # number_of_columns should be 1 but number_of_rows s the number of rows i inputs  \n",
    "              #\n",
    "              row               = tf.gather(inputs, 0)\n",
    "              number_of_columns = tf.size(row)\n",
    "              number_of_rows    = tf.cast(tf.divide(tf.size(inputs), number_of_columns), dtype = tf.int32)\n",
    "              number_of_rows = tf.size(inputs)  \n",
    "              #\n",
    "              n_components = tf.size(tf.strings.split([row]))\n",
    "              #\n",
    "              # split every row into tensor of words and result is, for example:\n",
    "              #  tf.Tensor(\n",
    "              # [['1'   'OR' '2'   'AND' '3'   'OR' '4'  ]\n",
    "              #  ['10'  'OR' '20'  'AND' '30'  'OR' '40' ]\n",
    "              #  ['100' 'OR' '200' 'AND' '300' 'OR' '400']], shape=(3, 7), dtype=string)\n",
    "              splitted_inputs = tf.cast(tf.strings.split(inputs), dtype = tf.string).to_tensor()  \n",
    "              splitted_inputs = tf.squeeze(splitted_inputs)  \n",
    "              #\n",
    "              # expression is the first row of the splitted_inouts, for example:\n",
    "              # expression:  tf.Tensor(['1' 'OR' '2' 'AND' '3' 'OR' '4'], shape=(7,), dtype=string)\n",
    "              # we use the first rwo to decide what to do for the entire splitted_inputs\n",
    "              expression = tf.gather(splitted_inputs, 0)\n",
    "              #\n",
    "              #####################################################################################################################\n",
    "              #\n",
    "              n = number_of_rows * n_components\n",
    "              stack   = tf.repeat([[\"\"]], repeats=[n], axis=0)\n",
    "              outputs = tf.repeat([[\"\"]], repeats=[number_of_rows], axis=0)\n",
    "              #\n",
    "              stack   = tf.reshape(stack,   shape = [number_of_rows, n_components]) \n",
    "              outputs = tf.reshape(outputs, shape = [number_of_rows, 1]) \n",
    "              stack_top_index = -1\n",
    "              #\n",
    "              # For example, we get the following stack sfter initialization:\n",
    "              # INITIAL STACK:    tf.Tensor(\n",
    "              #                              [['EMPTY']\n",
    "              #                               ['EMPTY']\n",
    "              #                               ['EMPTY']], shape=(3, 1), dtype=string)\n",
    "              # INITIAL OUTPUTs:  tf.Tensor(\n",
    "              #                              [['EMPTY']\n",
    "              #                               ['EMPTY']\n",
    "              #                               ['EMPTY']], shape=(3, 1), dtype=string)\n",
    "              ####################################################################################################################        \n",
    "              #\n",
    "              for i in tf.range(tf.size(expression)):\n",
    "                  #\n",
    "                  character = tf.gather(expression, i)\n",
    "                  #\n",
    "                  character_columns = tf.slice(splitted_inputs, [0, i], [number_of_rows, 1]) # get current column\n",
    "                  #\n",
    "                  #if character not in Operators:  # if an operand append in postfix expression\n",
    "                  #\n",
    "                  characterREG = character\n",
    "                  if tf.equal(character, \"(\") or tf.equal(character, \")\"):\n",
    "                     # we add \"\\\" to \"(\" or \")\" because otherwise regex_full_match consider it as command not as a symbol\n",
    "                     characterREG =  tf.strings.join([\"\\\\\", character]) \n",
    "                  #\n",
    "                  isoperator = tf.math.reduce_any(tf.strings.regex_full_match(['OR', 'AND', 'NOT', '(', ')'],  characterREG))\n",
    "                  #\n",
    "                  # tf.math.logical_not(isoperator) is TRUE when we have the number not a command\n",
    "                  if   tf.math.logical_not(isoperator): # not isoperator\n",
    "                       #\n",
    "                       # in this case we add current symbol for every row to outputs, \n",
    "                       # the previous code: output+= \" \" + character \n",
    "                       outputs = output_update(outputs, character_columns, number_of_rows)\n",
    "                       #\n",
    "                  elif tf.equal(character, '(' ):  # else Operators push onto stack\n",
    "                       # the previos command was: stack.append('(')\n",
    "                       # character_columns contains   \n",
    "                       # push()\n",
    "                       stack_top_index, stack = push_stack(stack, character_columns, number_of_rows, stack_top_index) \n",
    "                       #\n",
    "                  elif tf.equal(character, ')' ):\n",
    "\n",
    "                      #while stack and stack[-1]!= '(':\n",
    "                      #\n",
    "                      stack_top = top_one_row_stack(stack, stack_top_index)\n",
    "                      #\n",
    "                      while stack_not_empty(stack_top_index) and tf.not_equal(stack_top, \"(\"):\n",
    "                            #output+= \" \" + stack.pop()\n",
    "                            #\n",
    "                            stack_top_index, stack_2D_top = pop_stack(stack, number_of_rows, stack_top_index)\n",
    "                            #\n",
    "                            stack_top = top_one_row_stack(stack, stack_top_index)\n",
    "                            #\n",
    "                            # add saved top of the stack for every row to outputs\n",
    "                            outputs = output_update(outputs, stack_2D_top, number_of_rows)\n",
    "                            #\n",
    "                      #\n",
    "                      #stack.pop()  # remove \"(\"\n",
    "                      #\n",
    "                      stack_top_index, stack_2D_top = pop_stack(stack, number_of_rows, stack_top_index)\n",
    "                      # \n",
    "                  else: \n",
    "                      #\n",
    "                      stack_top = top_one_row_stack(stack, stack_top_index)\n",
    "                      #\n",
    "                      #while stack and stack[-1]!='(' and Priority[character]<=Priority[stack[-1]]:\n",
    "                      while stack_not_empty(stack_top_index) and \\\n",
    "                            tf.not_equal(stack_top, \"(\") and        \\\n",
    "                            tf.math.less_equal(table.lookup(character), table.lookup(stack_top)):    \n",
    "                            #\n",
    "                            #####  utput+= \" \" + stack.pop()\n",
    "                            #\n",
    "                            # we need stack_top here in order to get priority operation in Priority[stack[-1]]   \n",
    "                            stack_top = top_one_row_stack(stack, stack_top_index)\n",
    "                            #\n",
    "                            stack_top_index, stack_2D_top = pop_stack(stack, number_of_rows, stack_top_index)\n",
    "                            #\n",
    "                            # add saved top of the stack for every row to outputs\n",
    "                            outputs = output_update(outputs, stack_2D_top, number_of_rows) \n",
    "                            #   \n",
    "                      #### stack.append(character)\n",
    "                      stack_top_index, stack = push_stack(stack, character_columns, number_of_rows, stack_top_index)\n",
    "\n",
    "              #while stack:\n",
    "              # we add to uptput everything what is left in stack:\n",
    "              while stack_not_empty(stack_top_index):    \n",
    "                    #\n",
    "                    #output+= \" \" + stack.pop()\n",
    "                    #\n",
    "                    stack_top_index, stack_2D_top = pop_stack(stack, number_of_rows, stack_top_index)\n",
    "                    #\n",
    "                    # we add top os the stack to output\n",
    "                    outputs = output_update(outputs, stack_2D_top, number_of_rows)\n",
    "                    #\n",
    "              return outputs\n",
    "          # call\n",
    "          return infixToPostfix_FUN_AD(inputs)\n",
    "#\n",
    "class Precision_AD(keras.layers.Layer):\n",
    "      def __init__(self, **kwargs):\n",
    "          #\n",
    "          super(Precision_AD, self).__init__(**kwargs)\n",
    "          #\n",
    "          self.InfToPost = infixToPostfix_AD()\n",
    "      #  \n",
    "      def get_config(self):\n",
    "          config = super().get_config()\n",
    "          # save constructor args\n",
    "          #config['input_dim'] = self.input_dim\n",
    "          # \n",
    "          return config\n",
    "      #\n",
    "      #@tf.function\n",
    "      def call(self, inputs):\n",
    "          #\n",
    "          print(\"call precision: \", inputs)\n",
    "          inputs = self.InfToPost(inputs)\n",
    "          #\n",
    "          #inputs = self.DH(inputs)\n",
    "          #\n",
    "          return inputs \n",
    "##############################################################################################################\n",
    "inputs = tf.constant([[\"( 1 OR 2 )  AND 3 OR NOT ( 4 OR 5 )\"],[\"( 10 OR 20 ) AND 30 OR NOT ( 40 OR 50 )\"], [\"( 100 OR 200 ) AND 300 OR NOT ( 400 OR 500 )\"]])\n",
    "print(\"inputs: \")\n",
    "print(inputs)\n",
    "outputs = infixToPostfix_AD()(inputs)\n",
    "display(outputs)\n",
    "print(\"Next NN type prediction:\")\n",
    "#\n",
    "isTest = True\n",
    "#\n",
    "if isTest:\n",
    "   input_layer  = Input(shape = (1), dtype= tf.string)\n",
    "   #\n",
    "   output_layer = Precision_AD()(input_layer)  \n",
    "   #\n",
    "   model = Model(inputs = input_layer, outputs = output_layer)\n",
    "   #\n",
    "   print(\"!!!!!!!!!!!!!! prediction:\")\n",
    "   pred = model.predict(inputs, batch_size = 150)\n",
    "   print(pred)\n",
    "else:\n",
    "   model.save(\"infixToPostfix_AD\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select \" ( \" + cast(precison_1, string) + \" AND \" + cast(precison_1, string) + \" ) \" + \" OR \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test string input for Custom Layer\n",
    "class string_AD(keras.layers.Layer):\n",
    "      def __init__(self, **kwargs):\n",
    "          #\n",
    "          super(string_AD, self).__init__(**kwargs)\n",
    "      #  \n",
    "      def get_config(self):\n",
    "          config = super().get_config()\n",
    "          # save constructor args\n",
    "          #config['input_dim'] = self.input_dim\n",
    "          # \n",
    "          return config\n",
    "      #\n",
    "      #@tf.function\n",
    "      def call(self, inputs):\n",
    "          output = tf.map_fn(lambda x: tf.strings.join([x, ' Alex Dolia TEST']), inputs)\n",
    "          #output = inputs\n",
    "          return output\n",
    "#\n",
    "#\n",
    "inputs = tf.constant([[\"( 1 OR 2 )  AND 3 OR 4\"],[\"( 10 OR 20 ) AND 30 OR 40\"], [\"( 100 OR 200 ) AND 300 OR 400\"]])\n",
    "#\n",
    "input_layer  = Input(shape = (1), dtype= tf.string)\n",
    "#\n",
    "output_layer = string_AD()(input_layer)\n",
    "#\n",
    "model = Model(inputs = input_layer, outputs = output_layer)\n",
    "#\n",
    "pred = model.predict(inputs, batch_size = 150)\n",
    "\n",
    "pred "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
